{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from tensorflow.compat.v1.train import AdamOptimizer\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from collections import deque\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(512, activation='relu')\n",
    "        self.fc_out = Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = DQN(action_size=3, state_size=(84, 84, 4))\n",
    "global_target_model = DQN(action_size=3, state_size=(84, 84, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_memory = deque(maxlen=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./log.csv', 'w')\n",
    "file.write('episode,score,scoreMax,scoreAvg,memoryLength,epsilon,qAvg,avgLoss\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_update_model():\n",
    "        global_target_model.set_weights(global_model.get_weights())\n",
    "        global_model.save_weights(\"./save_model/model\", save_format=\"tf\")\n",
    "        print('global_model_update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Global_DQNAgent:\n",
    "    def __init__(self, action_size, state_size=(84, 84, 4)):\n",
    "        self.render = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.no_op_steps = 30\n",
    "\n",
    "        # 글로벌 모델 업대이트를 위한 옵티마이저 선언\n",
    "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
    "        \n",
    "        # 타깃 모델 초기화\n",
    "        global_update_model()\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.writer = tf.summary.create_file_writer('summary/breakout_dqn')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "        \n",
    "        self.threads = 1\n",
    "        \n",
    "        \n",
    "    # 쓰레드를 만들고, 학습을 하는 함수\n",
    "    def train(self):\n",
    "        # 쓰레드 수 만큼 Runner 클래스 생성\n",
    "        runners = [Runner(i) for i in range(self.threads)]\n",
    "\n",
    "        # 각 쓰레드 시작\n",
    "        for i, runner in enumerate(runners):\n",
    "            print(\"Start worker #{:d}\".format(i))\n",
    "            runner.start()\n",
    "\n",
    "            \n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(global_memory, self.batch_size)\n",
    "\n",
    "        history = np.array([sample[0][0] / 255. for sample in batch],\n",
    "                           dtype=np.float32)\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_history = np.array([sample[3][0] / 255. for sample in batch],\n",
    "                                dtype=np.float32)\n",
    "        dones = np.array([sample[4] for sample in batch])\n",
    "\n",
    "        \n",
    "        # 학습 파라메터\n",
    "        model_params = global_model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = global_model(history)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            target_predicts = global_target_model(next_history)\n",
    "\n",
    "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "            max_q = np.amax(target_predicts, axis=1)\n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "\n",
    "            # 후버로스 계산\n",
    "            error = tf.abs(targets - predicts)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error - quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "            self.avg_loss += loss.numpy()\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner(threading.Thread):\n",
    "    def __init__(self, num):\n",
    "        \n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = 3\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "        self.no_op_steps = 30\n",
    "\n",
    "        # 글로벌 모델 업대이트를 위한 옵티마이저 선언\n",
    "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
    "        \n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.writer = tf.summary.create_file_writer('summary/breakout_dqn')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "        \n",
    "        self.threads = 1\n",
    "        \n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.myNum = num\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        env = gym.make('BreakoutDeterministic-v4')\n",
    "        agent = DQNAgent(action_size=3, myNum=self.myNum)\n",
    "        \n",
    "        global_step = 0\n",
    "        score_avg = 0\n",
    "        score_max = 0\n",
    "        \n",
    "        # 불필요한 행동을 없애주기 위한 딕셔너리 선언\n",
    "        action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "        \n",
    "        num_episode = 50000\n",
    "        for e in range(num_episode):\n",
    "            done = False\n",
    "            dead = False\n",
    "            \n",
    "            step, score, start_life = 0, 0, 5\n",
    "            \n",
    "            # env 초기화\n",
    "            observe = env.reset()\n",
    "            \n",
    "            # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "            for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "                observe, _, _, _ = env.step(1)\n",
    "            \n",
    "            \n",
    "            # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "            state = pre_processing(observe)\n",
    "            history = np.stack((state, state, state, state), axis=2)\n",
    "            history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "            while not done:\n",
    "                if agent.render:\n",
    "                    env.render()\n",
    "                global_step += 1\n",
    "                step += 1\n",
    "                \n",
    "                # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "                action = agent.get_action(history)\n",
    "                # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "                real_action = action_dict[action]\n",
    "                \n",
    "                # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "                if dead:\n",
    "                    action, real_action, dead = 0, 1, False\n",
    "                \n",
    "                # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "                observe, reward, done, info = env.step(real_action)\n",
    "                # 각 타임스텝마다 상태 전처리\n",
    "                next_state = pre_processing(observe)\n",
    "                next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "                next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "                \n",
    "                agent.avg_q_max += np.amax(agent.model(np.float32(history / 255.))[0])\n",
    "                \n",
    "                if start_life > info['ale.lives']:\n",
    "                    dead = True\n",
    "                    start_life = info['ale.lives']\n",
    "                \n",
    "                score += reward\n",
    "                reward = np.clip(reward, -1., 1.)\n",
    "                # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "                # append_sample 수정\n",
    "                agent.append_sample(history, action, reward, next_history, dead)\n",
    "                \n",
    "                # 리플레이 메모리 크기가 정해놓은 수치에 도달한 시점부터 모델 학습 시작\n",
    "                if len(global_memory) >= agent.train_start:\n",
    "                    if agent.epsilon > agent.epsilon_end:\n",
    "                        agent.epsilon -= agent.epsilon_decay_step\n",
    "                    \n",
    "                    \n",
    "                    batch = random.sample(global_memory, self.batch_size)\n",
    "\n",
    "                    history = np.array([sample[0][0] / 255. for sample in batch],\n",
    "                                       dtype=np.float32)\n",
    "                    actions = np.array([sample[1] for sample in batch])\n",
    "                    rewards = np.array([sample[2] for sample in batch])\n",
    "                    next_history1 = np.array([sample[3][0] / 255. for sample in batch],\n",
    "                                        dtype=np.float32)\n",
    "                    dones = np.array([sample[4] for sample in batch])\n",
    "\n",
    "        \n",
    "                    # 학습 파라메터\n",
    "                    model_params = global_model.trainable_variables\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        # 현재 상태에 대한 모델의 큐함수\n",
    "                        predicts = global_model(history)\n",
    "                        one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "                        predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "                        # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "                        target_predicts = global_target_model(next_history1)\n",
    "\n",
    "                        # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "                        max_q = np.amax(target_predicts, axis=1)\n",
    "                        targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "\n",
    "                        # 후버로스 계산\n",
    "                        error = tf.abs(targets - predicts)\n",
    "                        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "                        linear_part = error - quadratic_part\n",
    "                        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "                        self.avg_loss += loss.numpy()\n",
    "\n",
    "                    # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "                    grads = tape.gradient(loss, model_params)\n",
    "                    self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "                    \n",
    "                    agent.update_model()\n",
    "                    \n",
    "                    \n",
    "                    # 일정 시간마다 global_model에서 가중치 받아오기\n",
    "                    if global_step % agent.update_target_rate == 0:\n",
    "                        global_update_model()\n",
    "                    \n",
    "                if dead:\n",
    "                    history = np.stack((next_state, next_state,\n",
    "                                        next_state, next_state), axis=2)\n",
    "                    history = np.reshape([history], (1, 84, 84, 4))\n",
    "                else:\n",
    "                    history = next_history\n",
    "                \n",
    "                if done:\n",
    "                    # 각 에피소드 당 학습 정보를 기록\n",
    "                    score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                    score_max = score if score > score_max else score_max\n",
    "                    \n",
    "                    if self.myNum == 0:\n",
    "                        # print(log)\n",
    "                        \n",
    "                        file = open('./log.csv', 'a')\n",
    "                        file.write(\"{:5d}\".format(e) + ',')\n",
    "                        file.write(\"{:4.1f}\".format(score) + ',')\n",
    "                        file.write(\"{:4.1f}\".format(score_max) + ',')\n",
    "                        file.write(\"{:4.1f}\".format(score_avg) + ',')\n",
    "                        file.write(\"{:5d}\".format(len(global_memory)) + ',')\n",
    "                        file.write(\"{:.3f}\".format(agent.epsilon) + ',')\n",
    "                        file.write(\"{:3.2f}\".format(agent.avg_q_max / float(step)) + ',')\n",
    "                        file.write(\"{:3.2f}\".format(agent.avg_loss / float(step)) + '\\n')\n",
    "                        file.close()\n",
    "                    \n",
    "\n",
    "                    agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "                    \n",
    "        # if self.myNum == 0:\n",
    "        #    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size, state_size=(84, 84, 4), myNum=0):\n",
    "        self.model = DQN(action_size, state_size)\n",
    "        self.update_model()\n",
    "        \n",
    "        # 예를 들어 앱실론을 점점 줄이며? 앱실론에 따라 행동 결정등 코드 추가\n",
    "        self.no_op_steps = 30\n",
    "        self.render = False\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 1000000.\n",
    "        # self.exploration_steps = 100000.\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.train_start = 50000\n",
    "        # self.train_start = 5000\n",
    "        self.update_target_rate = 10000\n",
    "        # self.update_target_rate = 1000\n",
    "        \n",
    "        if myNum == 0:\n",
    "            self.render=True\n",
    "    \n",
    "        \n",
    "    def update_model(self):\n",
    "        self.model.set_weights(global_model.get_weights())\n",
    "    \n",
    "    def get_action(self, history):  \n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(history)\n",
    "            return np.argmax(q_value[0])\n",
    "        \n",
    "    # global에 추가\n",
    "    def append_sample(self, history, action, reward, next_history, dead):\n",
    "        global_memory.append((history, action, reward, next_history, dead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_model_update\n",
      "Start worker #0\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n",
      "global_model_update\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_agent = Global_DQNAgent(action_size=3)\n",
    "    global_agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
